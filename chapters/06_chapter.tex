\label{chap:topics}

\section{Introduction}
RQs:
\begin{enumerate}
    \item How can we optimise the definition of topic in order to have enough details?
    \item How propaganda changes across topics and leanings?
    \item knowing the topic, does it make classification (prop-->leaning) easier?
\end{enumerate}

How this relates to other chapters


TODO: New structure (similar to chap 4):
- new ingredient: topic
- combination with previous ingredients


two documents:

Experiment 7: contains an analysis of different ways of extracting/using topics on my dataset. Here I show some breakdown by topic using some topics annotations in the dataset (AllSides topics), and also with the values from TextRazor (CoarseTopics, Fine-grained topics, but not yet using the hierarchical MediaTopics)

Expanding the topics of AllSides: also includes the analysis of MediaTopics (IPTC)


For the code, I have two parts:
https://github.com/MartinoMensio/textrazor-bulk-annotate which can be used to annotate a dataset (this is the script I used to annotate the datasets that you gave me in the past).
attached the code from a private repository (bcanalytics) which contains some functions to handle the taxonomy (file src/textrazor.py ) and to plot the sunburst diagrams (file experiments/text\_razor.ipynb)

\section{Topic analysis}

\subsection{Topic definition}

What it is

How it relates to concepts described in this thesis:
- headlines, clusters
- topic and anti-topic layers/words

\subsection{Topic at different granularities}

TODO

\subsubsection{Why TextRazor}
Regarding the validation of TextRazor, I am not aware of a benchmark done to check if it is better than other tools. It was suggested by Harith to use it, and I find that the data it provides is generally quite good (for topics and entities). But this is qualitatively, I didn’t do a benchmark or looked up for benchmarks. The assumption was that it’s a commercial product and it should be good.
I found some papers that claim to do benchmarks but I didn’t read them:
http://giusepperizzo.github.io/publications/Rizzo\_Erp-LREC2014.pdf for the entities
https://www.linkedin.com/pulse/google-nli-kill-market-linguistic-apis-review-yuri-kitin/ mentioning that TextRazor is useful because it links to Wikipedia/DBPedia

I chose TextRazor also because it’s the only one that I found that provides already hierarchical topics, the other ones always give topics that are not hierarchical or can be made hierarchical with external knowledge (e.g. using DBPedia to navigate broader topics)

\section{Topic and Propaganda and Leaning}

TODO introduce combined

\subsection{Breakdown of propaganda by topics}

This section contains analysis of how propaganda is distributed across topics and what differences topics contain in terms of propaganda.
Assumption: propaganda is used for certain topics in a very recognisable way to push for the ideas of the leaning of the source

RQ1: \emph{How propaganda changes across topics and leanings?}

Experiment 4.3: 
- topic breakdown: AllSides Topics, IPTC
- shapes of propaganda/sentiment


Types of shapes (propaganda)
(look at purple: propaganda)

Blue is sentiment (+ and -) and purple is propaganda. 
y axis in the fraction of terms marked as sentiment/propaganda.

\subsection{Predicting Leaning from topics and propaganda}

In this section we aim to analyse the relationship between topics, propaganda and leaning from a different perspective.

RQ2: \emph{knowing the topic, does it make classification (prop-->leaning) easier?}

7: topics+propaganda for political leaning prediction