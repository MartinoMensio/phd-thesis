\chapter{\statusgreen Discussions and Conclusions}
\label{chap:discussion}

This last chapter contains the final discussions and conclusions of this work.
We start with comparing the original motivations with the actual findings in Section~\ref{sec:discussion_goals}.
Then Section~\ref{sec:discussion_answers} presents our responses to the Research Questions and Section~\ref{sec:discussion_contributions} the main contributions.
In Section~\ref{sec:discussion_findings_implications} we discuss the implications of the contributions and we present the limitations and future works in Section~\ref{sec:discussion_limitations}.
We conclude with a final remark in Section~\ref{sec:discussion_conclusions}.








% \section{Introduction}
% \label{sec:discussion_intro}

% Recap of what was done in chapters

% We showed that...


% The scope of this chapter is to give the reader a conclusive overview of our research work. We analyse here what and how much we have done, and we compare this with the objectives that we originally set. For that, we summarise the work achieved in our thesis and how the research questions that we initially identified were answered. Out of those, we can then discuss which limitations our approach presents â€“ we are particularly interested in finding some plausible reasons causing them. The investigation of those can also help us in defining and proposing new solutions and extensions of our work, that we will undertake in the future. Some of them, as we will see, have already partly investigated and have resulted in new publications.


\section{\statusgreen Comparing with our Motivations}
\label{sec:discussion_goals}

% Start by reviewing your research questions or objectives. What were you hoping to achieve with your research? How do your findings address these questions or objectives?
With this thesis we had the general goal to understand the relationship between propaganda, similarities across documents, political leaning and topics.


% The goal of this thesis was to...
% The main motivation behind that was...
The main motivation behind that was to verify or refute that propaganda and leaning are strongly related, by using the additional dimensions of similarities and topics to reach the goal.


% Based on this main research hypothesis, i.e. XXXXX  
% our work focused on setting up a process in which we could

Here we compare with our original motivations, introduced in Chapter~\ref{chap:intro}, with the actual findings from our work:


\begin{enumerate} % computational point of view
    \item Verify or refute the relationship between political leaning and propaganda. %The literature assumes\todoAW{Very vague and sweeping. How would you justify this? (Or refute it?)} that the political ideology of news writers/editors conditions how news articles are written considering economic reasons, unconscious  assumptions and reporters-sources relations that are linked to political leanings~\citep{schudson2002news}. And these factors result in news articles that contain more or less subtle persuasion devices for the reader (one of which is propaganda). By analysing and quantifying these relationships, we can understand their weight. % contribution 1: compute weights of these relationships, understand better the relationships
    We have seen in Chapter~\ref{chap:political_sides} that the relationship between political leaning and propaganda exists.
    We computed in Chapter~\ref{chap:topics} the correlation of propaganda across the political spectrum, finding topics where propaganda is strictly related to the political leaning, resulting in a low correlation across the spectrum. For these topics, we can recognise more easily the propaganda of the left compared with the propaganda of the right. Therefore we verified the relationship, even with the current propaganda detection limitations (we will expand about them in the next sections).
    \item Improve some automated approaches that currently rely on just one input feature, such as political leaning classification using propaganda and topic features.
    Our analysis, based on a political leaning classifier, has shown that using propaganda as an input feature enables to improve the prediction results, and the improvement has passed the McNemar test for significance. Furthermore, in Chapter~\ref{chap:topics}, we introduced also the topics in our analysis and demonstrated how we can even improve more the results by using the topics.
    %As seen with Chapter~\ref{chap:political_sides} and~\ref{chap:topics}, the improvements observed are statistically significant, exploiting the relationship between these features. % contribution 2: improve F1 using mix of feature
    \item Discover problems and inconsistencies that can only be discovered with combined analyses (e.g., data imbalance). %This is a very important point when using \acrshort{ml}-based approaches, as improving the quality of the data is a key element. % contribution 3: 
    We have demonstrated, especially in Chapter~\ref{chap:political_sides}, a big problem of imbalance in the used dataset for propaganda detection. This imbalance is itself a direction for further research, to build a more balanced and comprehensive dataset to analyse propaganda.
\end{enumerate}


\section{\statusgreen Answers to the Research Questions}
\label{sec:discussion_answers}

% Multiple research questions arose from setting such objectives, namely:
We present here all the Research Questions together with the answers. The top-level questions are:

\begin{enumerate}[label={\textbf{RQ\arabic*:}},leftmargin=1.6cm]
\item To what extent do news articles about the same events differ? (Chapter 3)
\item To what extent can we automatically detect the persuasion techniques used in news articles? (Chapter 4)
\item To what extent could the use of persuasion techniques help identify the political leaning of a news article? (Chapter 5)
\item How does the use of propaganda differ across topics, and to what extent could this help determine the political leaning of articles? (Chapter 6)
\end{enumerate}


% On the basis of those research questions, we can now analyse the answers and contributions we did bring.
We present the answers for each question in the next subsections, together with the subquestions, to have a more detailed picture.


\subsection*{RQ1: To what extent do news articles about the same events differ?}

After comparing news articles covering the same events, we discovered that they can vary in several ways. Firstly, some articles may omit certain details while others confirm them. Secondly, different terms may be used to express the same detail. While it is relatively straightforward to analyze the first type of difference using similarity metrics, it is not sufficient for understanding the nuances of the second type. We need in this case to analyse the terms involved to go beyond the simple differences.
In more details, the answers to the subquestions are:

\begin{enumerate}[label={\textbf{RQ1.\arabic*:}},leftmargin=2cm]
    \item \emph{How news events are reported differently by multiple sources?} News sources present the same event in different ways, including different details that corroborate with other sources, omitting details that other sources instead report, changing the order of presentation of the details. We studied and measured these phenomena in this chapter, but we deem it necessary to study the choice of terms, by investigating what is the reason and what is the effect of deliberate or involountary choices.
    \item \emph{How could we identify what is unique for each version and what is common?} We reproduced the methodology of~\citet{bountouridis2018explaining} to detect omission and corroboration, %(Section~\ref{sec:cgs_cross_referencing})
    which is based on splitting the articles into sentences and comparing with similarity metrics. We extended the approach to analyse at the word level the variation of terms across multiple related articles, and compute the relative uniqueness of terms. %(Section~\ref{sec:cgs_clustering_and_differences}).
    \item \emph{To what extent can we automatically detect omission and corroboration across multiple articles?} The identified approach works better for detecting small differences between articles that have several parts in common. When the articles are too different, for example, if they consider totally different events or if the overlap of terms is too small, it becomes difficult to align the documents with similarity metrics and the results are less meaningful. We also find that our analysis is able to find terms that change but does not give an interpretation of what these changes mean. For example, we cannot currently distinguish between a loaded term versus a more neutral one. For this to be possible, we need the study of persuasion means (next RQ2).
    \item \emph{Which similarity metrics are best for detecting omission and corroboration?} We underlined % in Section~\ref{sec:cgs_similarity} 
    the importance of using semantic similarity metrics (USE, BERT) that go beyond the binary comparison (equal or different) of terms. In this way, we are more resistant to term changes and can identify better the related sentences across articles.
\end{enumerate}


\subsection*{RQ2: To what extent can we automatically detect the persuasion techniques used in news articles?}

Using the automated detection of persuasion in news articles, we have on one side a clearer idea of the relationship between persuasion and the variations that different news sources produce when reporting events.
On the other side, we have also a deeper understanding of the limitations of the current status of this automated detection, and the repercussions it can have when we use these methods in other tasks.
Computational detection of persuasion means is quite a recent research area, and with time and more resources (datasets and models) it could clearly improve.

More in details, we answer our subquestions:

\begin{enumerate}[label={\textbf{RQ2.\arabic*:}},leftmargin=2cm]
    \item \emph{To what extent could we automatically detect the persuasion techniques used by writers?} Sentiment is detected through lexicons and sentiment treebanks. Propaganda, instead, is recognised with deep learning models trained on fine-grained datasets. Both methodologies have their problems and provide results that cannot be fully trusted. For populism we do not have detection models, but for our initial findings it seems to be weakly correlated with propaganda.
    \item \emph{Which persuasion techniques are detected more frequently than others?} We observed that the most common techniques relate to loaded language (through propaganda and sentiment detection). However, the most common techniques are not the ones that differ the most when considering different versions of the same story. More subtle techniques, like Doubt and Appeal to fear, are the ones that emerge more when doing a comparative analysis between multiple versions of the same article.
    \item \emph{How do similar news articles differ in their use of persuasion techniques?} In some cases the words that change between multiple versions of the same detail, also cause a change in the detected persuasion. We managed to find that techniques like \texttt{Doubt}, \texttt{Appeal\_to\_fear-prejudice} and \texttt{Repetition} are the ones that are mostly affected by the variations in expressing the same detail. But in many cases, the differences between the articles cannot be described in terms of differences in detected sentiment or propaganda. %This is due to other types of variation: word choices/synonyms that are not the same word and do not apparently bring persuasion differences. Or also we have false positives in the detection (especially sentiment) that make it more difficult to see the changed persuasion.
    \item \emph{To what extent could persuasion techniques be used to identify related news articles?} By removing sentiment and/or propaganda, the clustering of related news articles improves slightly. This improvement is small but it is a sign that the non-topical layer is acting as noise/obstacle in recognising the topical layer. %This is a weak verification of our hypothesis presented in Chapter~\ref{ssec:lit_layers_of_info} where we assumed that the news articles are a mix between two layers of topical and non-topical words.
\end{enumerate}


\subsection*{RQ3: To what extent could the use of persuasion techniques help identify the political leaning of a news article?}

We observed and used the potential of persuasion analysis (more specifically, propaganda detection) to recognise the political leaning of news articles. The results are promising but are undermined by a high imbalance in the datasets used for training propaganda detection models.

\begin{enumerate}[label={\textbf{RQ3.\arabic*:}},leftmargin=2cm]
    \item \emph{How does persuasion vary across the political spectrum?} We detect less propaganda in the centre and more in the extremes. This was already hypothesised since more extreme points of view have usually stronger opinions which reflect in the language they use. Between Left and Right, the Right seems to be more propagandistic, especially with some techniques (\texttt{Flag-Waving} and \texttt{Slogans}). With the term analysis, we are able to relate to some contextual elements and for example recognise some slogans and terminology of the Left and Right. We see that different political leanings seem to be quite recognisable by observing the comparison of quantities and terms.
    \item \emph{To what extent can we predict the political leaning of a news article by observing the propaganda it uses?} We experimented with a classifier trained with different combinations of features, and the result is that propaganda features cannot themselves reach the recognition abilities of the BERT baseline. When combined with the baseline features, we see in some cases some small improvements, that test as significant according to McNemar tests. In these cases, the propaganda features help to correct some imbalances of the baseline classifier. However, the number of samples affected is quite small. This may be because BERT is already a very powerful model and any improvements over it have to be considered as a win already.
    \item \emph{How balanced are the current propaganda detection methods with regard to political leaning?} We then performed an analysis of the imbalance of the main datasets that are used for the English language to perform propaganda detection. The results are clearly indicating that most of them are imbalanced. It is not very clear what the effect of this imbalance is, but we can already see some of its effects by analysing the correlation with populism. Indeed, we see that some detected propaganda techniques result in being negatively correlated with populism. In our opinion, this is an indicator that more studies are needed to establish whether our hypotheses are wrong or if propaganda detection is having some problems.
\end{enumerate}


\subsection*{RQ4: How does the use of propaganda differ across topics, and to what extent could this help determine the political leaning of articles?}


Propaganda changes significantly across topics, especially when considering the terms used. Using the topic as an input feature for classifying political leaning is beneficial if used together with the propaganda features. The reason is that we can exploit some associations of topics and propaganda that make it easier to recognise the political leaning of news articles.

\begin{enumerate}[label={\textbf{RQ4.\arabic*:}},leftmargin=2cm]
    \item \emph{How does detected propaganda differ across polarising versus neutral topics?} We identified topics that contain more propaganda (polarising) than others. What does not change generally is the proportion between the techniques used. Loaded Language is always the most common, and there are a few exceptions where a specific technique is used in a certain topic outside its usual range.
    \item \emph{How does detected propaganda differ across political leaning in polarising and neutral topics?} When comparing across political leaning, some topics have a higher quantity of propaganda on a specific leaning. The internal proportion of techniques is preserved, but the specific terms have substantial differences. This means that for the polarising topics, we can easily recognise if an article is from the Left or the Right leaning.
    \item \textit{What are the effects of combining the propaganda features with the topic features, to recognise the leaning of a news article?} Adding the topic features to the propaganda features is useful to recognise the leaning of news articles, but not for all the topics. The small improvements show statistical significance, meaning that the interaction between the two dimensions is useful for the classification. As we observed in the previous RQ4.2, the polarising topics have distinctive terms, and we can exploit these differences in automated classification.
\end{enumerate}




\section{\statusgreen Contributions}
% \section{OVERALL CONTRIBUTION / Findings}
\label{sec:discussion_contributions}

The main contribution of this work is to bring more clarity about the relationship between propaganda, similarities across news articles, political leaning and topics.
The biggest part comes from the last Chapters~\ref{chap:political_sides} and~\ref{chap:topics} where we identify the data imbalance of the propaganda datasets and where we identify topics where the propaganda differs mostly between left and right.

% topics that are very loaded with propaganda (sexism, racism, discrimination, corruption) and as well topics with very low values (economy, health, defence)

For example, we find that some topics contain more propaganda specifically on one side of the political spectrum: economy, labour, disasters, court. In some other cases instead, propaganda appears uniformly across the spectrum (Fundamental Rights and Religion) but when we consider the terms we have significant differences (low correlation).


% A summary of your findings: This should be a clear and concise overview of the main results of your research. Be sure to highlight the most important findings and how they relate to your research questions or objectives.

% ONLY OVERALL. BUT EXPLAIN LINKS TO RQs and point to evidence.

More in details, we have three main contributions:

\begin{enumerate}
    \item Computed weights and better understanding of the relationships between the multiple dimensions of similarities, propaganda, leaning and topics. This is achieved with:
    \begin{enumerate}
        \item Chapter~\ref{sec:lp_relationship} contained an analysis of the relationship between persuasion and the word variations that different news sources produce when reporting events.
        \item Chapter~\ref{chap:political_sides} showed, by improving the classifier values of F1 metric, that propaganda and political leanings are slightly correlated. If they were not correlated, adding propaganda as an input feature would have no positive effect.
        \item Chapter~\ref{chap:topics} found that the topic is highly correlated with propaganda: some topics have very high levels of propaganda, while other ones are less polarizing and contain less of it. And considering the terms of propaganda, we find that they are very different between political leanings for some topics. %This not only means that the correlation between topics, leanings and propaganda is high, but it also means that
        %Certain topics have more propaganda on a specific leaning. This happens with topics that are more important from the considered point of view, or where the considered leaning is currently against the status quo.
        % \item Distribution of propaganda techniques is very similar across leanings for most of the topics (relative ratio of the quantity of techniques between themselves). Combined with the previous finding, it means that the quantities of the techniques scale proportionally across leanings in most of the topics.
        % \item Terms of propaganda can be quite different across political leaning in certain topics. For some of these topics, they already have an imbalance of total quantity (e.g. Left has more propaganda than Right), for some others, the quantity is very similar, but they differ in the terms used.
        % \item For a set of topics, it is easier to classify correctly the political leaning than in others. The easiest topics are the ones that are more polarising.
        % \item Adding propaganda features to the baseline model has a positive impact on prediction metrics on major topics, while for some topics instead, it has negative impacts.
        
    \end{enumerate}
    \item Improving F1 of the political leaning classifier using a mix of features. More specifically:
    \begin{enumerate}
        \item Chapter~\ref{chap:political_sides} showed that if we add propaganda features on a BERT-based classifier that predicts political leaning, we get slightly better results that are significant according to McNemar tests. In these cases, the propaganda features help to correct some imbalances of the baseline classifier. However, the number of samples affected is relatively small.
        \item Chapter~\ref{chap:topics}: added the topic and used it as a feature for the classifier. The result is an increase in the prediction metrics, small but significant.
    \end{enumerate}
    \item Highlighting the problem of imbalanced dataset for propaganda detection. This emerges from the analysis in different chapters:
    \begin{enumerate}
        \item Chapter~\ref{chap:linguistic_persuasion} resulted in a deeper understanding of the limitations of the current status of this automated detection (of propaganda), and the repercussions it can have when we use these methods in other tasks (e.g. overlap across news articles).
        % Computational detection of persuasion means is quite a recent research area, and with time and more resources (datasets and models) it could clearly improve
        \item Chapter~\ref{chap:political_sides} analysed and found a big imbalance of propaganda detection datasets considering political leaning. Almost only Right-leaning articles are used in the current literature.
        % First of all, they show how current propaganda detection is able to work with articles coming from very different political orientations. We think that the results here found are demonstrating quite good abilities to generalise from the relatively small datasets used for training propaganda.
        % And if we consider that a big proportion of the news used in our experiments comes from a different political leaning, we think that these are very promising results.
        % We were able to extract, analyse and link to our external knowledge of the events and ideologies. This is a very positive outcome.
        \item Chapter~\ref{chap:topics} expanded on this imbalance and finds topics that contain more or less propaganda generally or in a specific political leaning.
    \end{enumerate}

    
    % \item Chapter 3: As already denoted by the work of~\citet{bountouridis2018explaining}, we confirm a positive correlation between corroboration and credibility of news outlets and a negative correlation between omission and credibility. We went one step further, by being able to automatically find the specific words that change between multiple news articles, and identify the degree of uniqueness of them. We think that this is beneficial for many downstream tasks, such as showing to the user during annotation tasks or even when consuming news online. 
    % \item Observing similarities between multiple documents is made very difficult by linguistic variations. We experimented with models (e.g. \acrshort{use}) that are more resistant to words that carry similar meanings and are a better fit for doing this type of analysis.


    % \item Chapter 4: chapter highlighted how, by integrating the automated detection of persuasion in news articles, we have on one side a clearer idea of the relationship between persuasion and the variations that different news sources produce when reporting events.
    % \item On the other side, we have also a deeper understanding of the limitations of the current status of this automated detection, and the repercussions it can have when we use these methods in other tasks.
    % Computational detection of persuasion means is quite a recent research area, and with time and more resources (datasets and models) it could clearly improve

    % \item Chapter 5: % - positive: generalisation quite good considering datasets and results obtained
    % First of all, they show how current propaganda detection is able to work with articles coming from very different political orientations. We think that the results here found are demonstrating quite good abilities to generalise from the relatively small datasets used for training propaganda.
    % And if we consider that a big proportion of the news used in our experiments comes from a different political leaning, we think that these are very promising results.
    % We were able to extract, analyse and link to our external knowledge of the events and ideologies. This is a very positive outcome.

    % - finding TOPICS where difference is more accentuated
    % At the same time, this chapter has also helped us to find a direction for more investigation. The results found here represent the whole dataset. We would like to know if we can spot more in details the differences of propaganda when we consider specific topics separately. Propaganda may differ between political leanings more when we select certain topics, and we would like to know both which topics and also the outcomes of such a detailed analysis. And for conducing this experimentation, we need to consider the \emph{topics} of the articles as an additional element.

    % \item Chapter 6: \item We need fine-grained topic to be able to see differences. The coarse topics show similar propaganda across topics and leanings. The more we use fine-grained topics, the more differences we are able to see. But at the same time, we lose support (fewer articles specific to the topics, and the filtering becomes too narrow). We need a tradeoff between granularity (high to see good differences) and support (significance of results).
    % \item Certain topics have more propaganda on a specific leaning. This happens with topics that are more important from the considered point of view, or where the considered leaning is currently against the status quo.
    % \item Distribution of propaganda techniques is very similar across leanings for most of the topics (relative ratio of the quantity of techniques between themselves). Combined with the previous finding, it means that the quantities of the techniques scale proportionally across leanings in most of the topics.
    % \item Terms of propaganda can be quite different across political leaning in certain topics. For some of these topics, they already have an imbalance of total quantity (e.g. Left has more propaganda than Right), for some others, the quantity is very similar, but they differ in the terms used.
    % \item For a set of topics, it is easier to classify correctly the political leaning than in others. The easiest topics are the ones that are more polarising.
    % \item Adding propaganda features to the baseline model has a positive impact on prediction metrics on major topics, while for some topics instead, it has negative impacts.
    % \item Encoding the topic information and using it as a feature, helps increase the prediction metrics of a leaning classifier. The improvements are small but significant.

    
    % \item Topics where current automated propaganda detection is more problematic (TODO) 
    % \item Link between propaganda and political leaning is weak (not enough to identify leaning by just looking at the propaganda techniques) â†’ against HYP1
    % \item Imbalance is / is-not a problem for propaganda detection
\end{enumerate}



\section{\statusgreen Implications of the Contributions}
\label{sec:discussion_findings_implications}

% A discussion of the implications of your findings: This is where you will interpret your findings and explain their significance. Consider how your findings contribute to the existing body of knowledge in your field. What new insights do they provide? What are the implications for practice or policy?

Our contributions have implications for two groups of potential shareholders:

\begin{enumerate}
    \item Computational Researchers:
    \begin{enumerate}
        \item We verify the relationship between political leaning and propaganda with comparative analysis, correlation analysis and the work on features for the classifier of political leaning. By proving that this relationship exists, we bring into focus that more interdisciplinary research is needed between these two research topics (propaganda detection and political leaning classification), to understand the interaction between them even further.  
        %The literature assumes that the political ideology of news writers/editors conditions how news articles are written considering economic reasons, unconscious assumptions and reporters-sources relations that are linked to political leanings~\citep{schudson2002news}. And these factors result in news articles that contain more or less subtle persuasion devices for the reader (one of which is propaganda). By analysing and quantifying these relationships, we can understand their weight. % contribution 1: compute weights of these relationships, understand better the relationships
        \item By using together these multiple variables, we improved some automated approaches that currently rely on just one input feature, such as political leaning classification using propaganda and topic features. By exploiting the relationship between these features, we can help \acrshort{ml} models to have higher values for the target metrics in other applications. % contribution 2: improve F1 using mix of feature
        \item We discovered a big problem of data imbalance on the datasets used for training propaganda detection models. This implies that the next studies should tackle this problem with more balanced datasets in order to reduce this problem. We identified the leaning and topics where the considered dataset is lacking articles. % is a very important point when using \acrshort{ml}-based approaches, as improving the quality of the data is a key element. % contribution 3: We highlight the problem of imbalanced dataset for propaganda detection
    \end{enumerate}%\todoHA{Fits better in a discussion section at the end of the thesis}
    \item Users reading the news:
    \begin{enumerate}
        \item By using the methodologies explored in this thesis, there is the potential to build applications that link and show similarities to other articles on how a certain event is covered. This becomes extremely important when we compare with articles from other political leanings which may have a different angle on the events described.
        \item Similar tools can be built to highlight propaganda, to make the users conscious of the techniques used in the articles at the word level. It is crucial for this analysis to be as accurate as possible, and that is why the insights from this thesis should drive the development of balanced datasets as noted above. 
        \item Show to the user, with the help of tools, how the propaganda of the news articles varies across news sources when describing the same events. With this analysis, we hope to make the users more informed about the manipulation they are faced with and be less manipulable.
    \end{enumerate}
\end{enumerate}

\section{\statusgreen Limitations and Future Work}
\label{sec:discussion_limitations}

During this research path that lasted more than 3 years, we have encountered many limitations, both technical and conceptual.

% Limitations of your research: It is important to be aware of the limitations of your research. This will help to put your findings in context and avoid overstating their significance. What factors could have influenced your results? What areas of your research could have been improved?

% Datasets and US
The first limitation, that comes from the datasets used, is that we use news articles that come from the US. While this is a relevant country for the English language, we think that it is limiting that the news articles do not come from other English-speaking countries such as the UK or international news.
This implies that most of the events covered by the articles in the dataset are specific to the US (e.g. Trump, elections).
This limitation applies to the datasets of political leaning (AllSides \texttt{baly}, \texttt{nela-gt}) and even to the propaganda datasets (TSHP-17, QProp and PTC). In Chapter~\ref{chap:common_ground_search} we made use of the Google News dataset, which instead we collected from Google News UK. This dataset did not have this limitation. However, it had many other problems such as the difficulty in retrieving data with scraping, and also possible licensing issues with sharing the articles (scraping for research is allowed by law, but sharing the content is not). Instead, we continued to use already public datasets to avoid these issues.

This limitation can be surely targeted by future works, collecting articles from different nations and building a more global dataset both for political leaning and for propaganda detection. In both cases, behind the technical difficulty of retrieving cleaned articles, there is a need to annotate the data and therefore manual effort is required. For political leaning, researchers can rely on distant supervision: using the leaning of the news source as the label for the articles. But for fine-grained propaganda detection, the articles need to be analysed by trained expert annotators, who need to correctly identify the propaganda techniques. The annotations need to pass quality checks by computing properly inter-annotator agreement. With such comprehensive datasets, more comparative analysis would be possible across nations.

The second big limitation of this work is that we are not training cutting-edge models to perform propaganda detection or political leaning classification.
For fine-grained propaganda detection, we are instead relying on pre-trained models~\citep{da2019fine}.
Instead, for political leaning, we are training a neural network that has 2 layers maximum. We use this network with pre-computed features (BERT encodings, TF-IDF of different subsets of terms, numerical features representing quantity of propaganda techniques) and therefore the training times are reduced significantly.
In the last years, there has been a race to huge and complex networks, that require significant amounts of computational power, besides the cost and time to run experiments (and environmental consequences).
We stick to relatively simple models, applying Occam's principle: we need to observe improvements on simple models first.
Our goal is to demonstrate the relationship between the involved features, and not primarily to get results that compete with the state of the art.

This limitation can quite simply be faced using more recent and complex models, potentially involving generative models such as GPT-4~\citep{openai2023gpt4} to generate training data or even to classify documents and terms.



\section{\statusgreen Final Remarks}
\label{sec:discussion_conclusions}

In this dissertation, we presented propaganda in relationships with multiple dimensions, such as political leaning, similarities across documents, and topics. Our analysis of these relationships was approached from multiple perspectives, which posed significant challenges. Nonetheless, these challenges served to make this doctoral work particularly engaging and rewarding.

From our contribution, we hope that the communities of researchers working on several computational news analysis tasks (e.g., propaganda detection, parallel news analysis and political leaning classification) can join their efforts and proceed in a research direction that would exploit the relationships that we described here in this work.
